{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import json\n",
    "# -------------------------\n",
    "# 1. 环境定义\n",
    "# -------------------------\n",
    "def load_concatenated_json(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing one or more concatenated JSON objects\n",
    "    and merges them into a single, valid data structure.\n",
    "    \"\"\"\n",
    "    decoder = json.JSONDecoder()\n",
    "    data_parts = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        idx = 0\n",
    "        while idx < len(content):\n",
    "            while idx < len(content) and content[idx].isspace():\n",
    "                idx += 1\n",
    "            if idx == len(content):\n",
    "                break\n",
    "            try:\n",
    "                obj, end = decoder.raw_decode(content, idx)\n",
    "                data_parts.append(obj)\n",
    "                idx = end\n",
    "            except json.JSONDecodeError:\n",
    "                break\n",
    "\n",
    "    if not data_parts:\n",
    "        return None\n",
    "\n",
    "    # Merge the parts\n",
    "    final_data = {k: v for k, v in data_parts[0].items() if k != 'traces'}\n",
    "    all_traces = [trace for part in data_parts for trace in part.get('traces', [])]\n",
    "    \n",
    "    final_data['traces'] = all_traces\n",
    "    final_data['num_traces'] = len(all_traces)\n",
    "    \n",
    "    return final_data\n",
    "class DeepConfEnv:\n",
    "    def __init__(self, traces):\n",
    "        self.traces = traces\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.idx = np.random.randint(len(self.traces))\n",
    "        self.trace = self.traces[self.idx]\n",
    "        self.conf_curve = np.array(self.trace['group_conf'])\n",
    "        self.correct = self.trace['ground_truth_correct']\n",
    "        self.t = 0\n",
    "        return np.array([self.conf_curve[self.t]])\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "\n",
    "        if action == 1 or self.t == len(self.conf_curve) - 1:\n",
    "            done = True\n",
    "            # 奖励设计\n",
    "            if self.correct:\n",
    "                reward = 1.0\n",
    "            else:\n",
    "                reward = -1.0\n",
    "            reward -= 0.01 * self.t  # 稍微惩罚长trace\n",
    "        else:\n",
    "            self.t += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        next_state = np.array([self.conf_curve[self.t]]) if not done else np.zeros(1)\n",
    "        return next_state, reward, done\n",
    "\n",
    "# -------------------------\n",
    "# 2. Policy 网络\n",
    "# -------------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------------\n",
    "# 3. 训练 RL (REINFORCE)\n",
    "# -------------------------\n",
    "def train_rl(traces, num_episodes=1000, gamma=0.99, lr=1e-3):\n",
    "    env = DeepConfEnv(traces)\n",
    "    policy = PolicyNet()\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        states, actions, rewards = [], [], []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "            probs = policy(state_t)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done = env.step(action.item())\n",
    "\n",
    "            states.append(state_t)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # 计算回报\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "\n",
    "        # policy gradient 更新\n",
    "        loss = 0\n",
    "        for s, a, R in zip(states, actions, returns):\n",
    "            probs = policy(s)\n",
    "            log_p = torch.log(probs[0, a])\n",
    "            loss -= log_p * R\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return policy\n",
    "\n",
    "# -------------------------\n",
    "# 4. 评估\n",
    "# -------------------------\n",
    "def evaluate_policy(policy, traces, num_samples=200):\n",
    "    y_true, y_pred = [], []\n",
    "    stop_positions = []\n",
    "\n",
    "    for i in range(min(num_samples, len(traces))):\n",
    "        trace = traces[i]\n",
    "        conf_curve = np.array(trace['group_conf'])\n",
    "        correct = trace['ground_truth_correct']\n",
    "\n",
    "        t = 0\n",
    "        while t < len(conf_curve) - 1:\n",
    "            s = torch.FloatTensor([[conf_curve[t]]])\n",
    "            probs = policy(s)\n",
    "            action = torch.argmax(probs).item()\n",
    "            if action == 1:\n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "        y_true.append(int(correct))\n",
    "        y_pred.append(int(correct))  # 暂时假设 early stop 不影响 correctness\n",
    "        stop_positions.append(t)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Wrong\", \"Correct\"])\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(\"RL Policy Early Stop Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return stop_positions\n",
    "\n",
    "# -------------------------\n",
    "# 5. 可视化若干条 Trace\n",
    "# -------------------------\n",
    "def plot_sample_traces(traces, policy, num_examples=5):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for i in range(num_examples):\n",
    "        trace = traces[i]\n",
    "        conf_curve = np.array(trace['group_conf'])\n",
    "        correct = trace['ground_truth_correct']\n",
    "\n",
    "        t = 0\n",
    "        while t < len(conf_curve) - 1:\n",
    "            s = torch.FloatTensor([[conf_curve[t]]])\n",
    "            probs = policy(s)\n",
    "            action = torch.argmax(probs).item()\n",
    "            if action == 1:\n",
    "                break\n",
    "            t += 1\n",
    "\n",
    "        color = \"green\" if correct else \"orange\"\n",
    "        plt.plot(conf_curve, color=color, alpha=0.6)\n",
    "        plt.axvline(t, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Group Confidence\")\n",
    "    plt.title(\"RL Policy Early Stop Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# 6. 主流程\n",
    "# -------------------------\n",
    "data = load_concatenated_json('trace_data/aime_2025_0_full.jsonl')\n",
    "traces = data['traces']\n",
    "\n",
    "policy = train_rl(traces, num_episodes=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_positions = evaluate_policy(policy, traces)\n",
    "plot_sample_traces(traces, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
